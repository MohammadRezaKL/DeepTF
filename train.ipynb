{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qapObWbs5TJK"
      },
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import scipy.io\n",
        "from sklearn import metrics\n",
        "import pandas as pd\n",
        "import os\n",
        "os.environ['THEANO_FLAGS'] = \"device=cuda0,force_device=True,floatX=float32,gpuarray.preallocate=0.3\"\n",
        "import theano\n",
        "print(theano.config.device)\n",
        "from keras.layers import Embedding\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Layer, merge, Input, Concatenate, Reshape, concatenate,Lambda,multiply,Permute,Reshape,RepeatVector\n",
        "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
        "from keras.layers.pooling import GlobalMaxPooling1D\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras import optimizers\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "from keras.layers import GRU\n",
        " \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJsn34lu5v4T"
      },
      "source": [
        "sequence_input = Input(shape=(1000,4))\n",
        " \n",
        "# Convolutional Layer\n",
        "output = Conv1D(320,kernel_size=26,padding=\"valid\",activation=\"relu\")(sequence_input)\n",
        "output = MaxPooling1D(pool_size=13, strides=13)(output)\n",
        "output = Dropout(0.2)(output)\n",
        " \n",
        "#Attention Layer\n",
        "attention = Dense(1)(output)\n",
        "attention = Permute((2, 1))(attention)\n",
        "attention = Activation('softmax')(attention)\n",
        "attention = Permute((2, 1))(attention)\n",
        "attention = Lambda(lambda x: K.mean(x, axis=2), name='attention',output_shape=(75,))(attention)\n",
        "attention = RepeatVector(320)(attention)\n",
        "attention = Permute((2,1))(attention)\n",
        "output = multiply([output, attention])\n",
        " \n",
        "#GRU Layer\n",
        "output = Bidirectional(GRU(320,return_sequences=True))(output)\n",
        "output = Dropout(0.5)(output)\n",
        " \n",
        " \n",
        "flat_output = Flatten()(output)\n",
        " \n",
        "#FC Layer\n",
        "FC_output = Dense(695)(flat_output)\n",
        "FC_output = Activation('relu')(FC_output)\n",
        " \n",
        "#Output Layer\n",
        "output = Dense(690)(FC_output)\n",
        "output = Activation('sigmoid')(output)\n",
        " \n",
        "model = Model(inputs=sequence_input, outputs=output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22ewQd_MuNP1"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=0.007)\n",
        "loss_fn = keras.losses.BinaryCrossentropy(from_logits=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rOYKLYFxKrU"
      },
      "source": [
        "batch_size = 100\n",
        "trainmat = h5py.File('/content/deepsea_train/train.mat')\n",
        "x = trainmat['trainxdata']\n",
        "y = trainmat['traindata']\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.0079)\n",
        "loss_fn = keras.losses.BinaryCrossentropy(from_logits=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZIQRkFoxwy4"
      },
      "source": [
        "epochs = 60\n",
        "loss_value = 0.0\n",
        "for epoch in range(epochs):\n",
        "  print(\"\\nStart of epoch %d\" % (epoch,))\n",
        " \n",
        "  for i in range(44000):\n",
        "    x_batch_train = np.transpose(np.array(x[:,:,i * 100 : (i+1) * 100]),axes=(2,0,1))\n",
        "    y_batch_train = np.transpose(np.array(y[:,i * 100 : (i+1) * 100]),axes=(1,0))\n",
        "    y_batch_train = y_batch_train[:,125:815]\n",
        " \n",
        "    with tf.GradientTape() as tape:\n",
        " \n",
        "      logits = model(x_batch_train, training=True)\n",
        " \n",
        "      loss_value = loss_fn(y_batch_train, logits)\n",
        " \n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        " \n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        " \n",
        "    if(i % 43999 == 0):\n",
        "      print(\"Training loss at epoch %d: %.4f\" % (epoch, float(loss_value)))\n",
        "    if(i == 43999):\n",
        "      model.save('/content/DeepTF.h5')  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}